spark-bench = {
  spark-submit-config = [
    {
      spark-home = "/usr/hdp/current/spark2-client"
      spark-args = {
        master = "yarn"
      }
      workload-suites = [
        {
          descr = "Multiple runs of DefaultGenerator with defaults memory configurations"
          benchmark-output = "wasbs://spark-bench@allstoragesv2.blob.core.windows.net/spark-bench/v1.1/output/default-generator/adl/10files/csv/default/data.csv"
          save-mode = "overwrite"
          workloads = [
            {
              name = "default-generator"
              rows = 10
              cols = 24
              output = "adl://cdcbigdataall.azuredatalakestore.net/spark-bench/v1.1/data/default-generator/10files/default/10Ko.csv"
              str = "Hello default-generator 10 and default confs!"
            },
            {
              name = "default-generator"
              rows = 1000
              cols = 24
              output = "adl://cdcbigdataall.azuredatalakestore.net/spark-bench/v1.1/data/default-generator/10files/default/1Mo.csv"
              str = "Hello default-generator 1000 and default confs!"
            }
          ]
        }
      ]
    },
    {
      spark-home = "/usr/hdp/current/spark2-client"
      spark-args = {
        master = "yarn" // replace with local, yarn, whatever
        executor-memory = 5G
        driver-memory = 5G
        num-executors = 8
      }
      workload-suites = [
        {
          descr = "Multiple runs of DefaultGenerator with defaults memory configurations"
          benchmark-output = "wasbs://spark-bench@allstoragesv2.blob.core.windows.net/spark-bench/v1.1/output/default-generator/adl/10files/csv/medium/data.csv"
          save-mode = "overwrite"
          workloads = [
            {
              name = "default-generator"
              rows = 10
              cols = 24
              output = "adl://cdcbigdataall.azuredatalakestore.net/spark-bench/v1.1/data/default-generator/10files/custom/10Ko.csv"
              str = "Hello default-generator 10 and custom confs!"
            },
            {
              name = "default-generator"
              rows = 1000
              cols = 24
              output = "adl://cdcbigdataall.azuredatalakestore.net/spark-bench/v1.1/data/default-generator/10files/custom/1Mo.csv"
              str = "Hello default-generator 1000 and custom confs!"
            },
            {
              name = "default-generator"
              rows = 1000000
              cols = 24
              output = "adl://cdcbigdataall.azuredatalakestore.net/spark-bench/v1.1/data/default-generator/10files/custom/1Go.csv"
              str = "Hello default-generator 1000000 and custom confs!"
            }
          ]
        }
      ]
    },
    {
      spark-home = "/usr/hdp/current/spark2-client"
      spark-args = {
        master = "yarn" // replace with local, yarn, whatever
        executor-memory = 10G
        driver-memory = 20G
        num-executors = 12
      }
      workload-suites = [
        {
          descr = "Multiple runs of DefaultGenerator with big memory configurations"
          benchmark-output = "wasbs://spark-bench@allstoragesv2.blob.core.windows.net/spark-bench/v1.1/output/default-generator/adl/10files/csv/big/data.csv"
          save-mode = "overwrite"
          workloads = [
            {
              name = "default-generator"
              rows = 10
              cols = 24
              output = "adl://cdcbigdataall.azuredatalakestore.net/spark-bench/v1.1/data/default-generator/10files/big/10Ko.csv"
              str = "Hello default-generator 10 and big confs!"
            },
            {
              name = "default-generator"
              rows = 1000
              cols = 24
              output = "adl://cdcbigdataall.azuredatalakestore.net/spark-bench/v1.1/data/default-generator/10files/big/1Mo.csv"
              str = "Hello default-generator 1000 and big confs!"
            },
            {
              name = "default-generator"
              rows = 1000000
              cols = 24
              output = "adl://cdcbigdataall.azuredatalakestore.net/spark-bench/v1.1/data/default-generator/10files/big/1Go.csv"
              str = "Hello default-generator 1000000 and big confs!"
            },
            {
              name = "default-generator"
              rows = 100000000
              cols = 24
              output = "adl://cdcbigdataall.azuredatalakestore.net/spark-bench/v1.1/data/default-generator/10files/big/100Go.csv"
              str = "Hello default-generator 100000000 and big confs!"
            },
            {
              name = "default-generator"
              rows = 100000000
              cols = 24
              output = "adl://cdcbigdataall.azuredatalakestore.net/spark-bench/v1.1/data/default-generator/10files/big/1To.csv"
              str = "Hello default-generator 1000000000 and big confs!"
              numfiles = 1
            },
            {
              name = "default-generator"
              rows = 100000000
              cols = 24
              output = "adl://cdcbigdataall.azuredatalakestore.net/spark-bench/v1.1/data/default-generator/10files/big/1To.csv"
              str = "Hello default-generator 1000000000 and big confs!"
              writemode = "append"
              numfiles = 1
            },
            {
              name = "default-generator"
              rows = 100000000
              cols = 24
              output = "adl://cdcbigdataall.azuredatalakestore.net/spark-bench/v1.1/data/default-generator/10files/big/1To.csv"
              str = "Hello default-generator 100000000 and big confs!"
              writemode = "append"
              numfiles = 1
            },
            {
              name = "default-generator"
              rows = 100000000
              cols = 24
              output = "adl://cdcbigdataall.azuredatalakestore.net/spark-bench/v1.1/data/default-generator/10files/big/1To.csv"
              str = "Hello default-generator 100000000 and big confs!"
              writemode = "append"
              numfiles = 1
            },
            {
              name = "default-generator"
              rows = 100000000
              cols = 24
              output = "adl://cdcbigdataall.azuredatalakestore.net/spark-bench/v1.1/data/default-generator/10files/big/1To.csv"
              str = "Hello default-generator 100000000 and big confs!"
              writemode = "append"
              numfiles = 1
            },
            {
              name = "default-generator"
              rows = 100000000
              cols = 24
              output = "adl://cdcbigdataall.azuredatalakestore.net/spark-bench/v1.1/data/default-generator/10files/big/1To.csv"
              str = "Hello default-generator 100000000 and big confs!"
              writemode = "append"
              numfiles = 1
            },
            {
              name = "default-generator"
              rows = 100000000
              cols = 24
              output = "adl://cdcbigdataall.azuredatalakestore.net/spark-bench/v1.1/data/default-generator/10files/big/1To.csv"
              str = "Hello default-generator 100000000 and big confs!"
              writemode = "append"
              numfiles = 1
            },
            {
              name = "default-generator"
              rows = 100000000
              cols = 24
              output = "adl://cdcbigdataall.azuredatalakestore.net/spark-bench/v1.1/data/default-generator/10files/big/1To.csv"
              str = "Hello default-generator 100000000 and big confs!"
              writemode = "append"
              numfiles = 1
            },
            {
              name = "default-generator"
              rows = 100000000
              cols = 24
              output = "adl://cdcbigdataall.azuredatalakestore.net/spark-bench/v1.1/data/default-generator/10files/big/1To.csv"
              str = "Hello default-generator 100000000 and big confs!"
              writemode = "append"
              numfiles = 1
            },
            {
              name = "default-generator"
              rows = 100000000
              cols = 24
              output = "adl://cdcbigdataall.azuredatalakestore.net/spark-bench/v1.1/data/default-generator/10files/big/1To.csv"
              str = "Hello default-generator 100000000 and big confs!"
              writemode = "append"
              numfiles = 1
            }
          ]
        }
      ]
    }
  ]
}